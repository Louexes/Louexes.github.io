---
title: "Predicting Weather Patterns with LSTM Time Series Model"
excerpt: "Application of LSTM model to capture short and long term weather trends for the Amsterdam region.<br/><img src='/images/weather.png'>"
collection: portfolio
---

<h1>LSTM Model Architecture Explained</h1>


Long Short-Term Memory networks (LSTMs) are a type of recurrent neural network (RNN) architecture designed to overcome the vanishing gradient problem common when training RNNs. The vanishing gradient problem is a major challenge in deep learning. It is a phenomenon where the gradients in a neural network become extremely small and effectively vanish, meaning they contribute little to the update of the network's weights during backpropogation. This is particularly problematic in multi-layer neural networks, as earlier layers of the network learn very slowly or not at all, leading to poor predictive performance and inability to identify patterns.

LSTMs address this issue by introducing a more complex memory mechanism within each cell. Unlike traditional RNNs which have a simple recurrent connection, LSTMs introduce a memory cell which is a container that can hold information for an extended period. This makes LSTMs ideal for learning long term dependencies in sequential data, such as those found in natural language processing and time series analysis. 

<h2>The Memory Cell</h2>

The key ingredient of LSTMs is the memory cell. It is controlled by three gates: the input gate, the forget gate and the output gate. These gates determine which information to add, remove and output from the memory cell. The input gate controls what information is added to the memory cell. The forget gate controls what information is removed from the memory cell. And the output gate controls what information is output from the memory cell. This is precisely what gives LSTM networks the capability to selectively retain and discard infromation as it flows through the network, allowing for the network to learn short and long-term dependencies.

<h3>The Forget Gate</h3>

The information that is no longer useful in the cell state is removed with the forget gate.

$$f_{t} = W_{\mathbb{f}}(h_{t-1}, x_{t}) + b_{\mathbb{f}}$$

Inputs: $x_{t}$ (some input x at time t) and $h_{t-1}$ (previous cell output)\
$W_{\mathbb{f}}$: Weight matrix \
$b_{\mathbb{f}}$: Bias term 

The resultant $f_{t}$ is passed through an activation function $\sigma$ and gives a binary output. If output is 1 piece of information is retained and if output is 0 the piece of information is forgotten.

<h3>The Input Gate</h3>

The input gate is important for the addition of useful information to the cell state. Similarly to the forget gate, the information is regulated using the sigmoid function and values are filted to remembered using inputs $x_{t}$ and $h_{t-1}$. Then a vector is created using the tanh function, giving an output from -1 to +1 containing all possible values from $x_{t}$ and $h_{t-1}$. Finally the values of the vector and regulated values are multiplied to obtain the useful information. This is captured by the following equations.

$$i_{t} = W_{i}(h_{t-1}, x_{t}) + b_{\mathbb{i}}$$

$$\hat{C}_{t} = tanh(W_{c}(h_{t-1}, x_{t}) + b_{\mathbb{c}})$$

The previous state $C_{t-1}$ is the multiplied by $\mathbb{f}_{t}$, which disregards the information previously selected to be ignored. Next the term $i_{c} \cdot \hat{C}_{t}$ is included which represents the updated candidate values. The final output is:

$$\hat{C}_{t} = \mathbb{f}_{t}C_{t-1} + i_{c}\hat{C}_{t} $$

<h3>The Output Gate</h3>

The output gate is required to extract useful information from the current cell state $C_{t}$ to be presented as output. First, a vector is generated by applying the tanh function on the cell. Then, the information is regulated via the sigmoid function and values to be remembered are filtered using inputs $x_{t}$ and $h_{t-1}$. Finally, the values of the vector and regulated values are multiplied to be sent as an output and input to the next cell. The equation is given as:
$$o_{t} = W_{o}(h_{t-1},x_{t}) + b_{0}$$




![image.png](attachment:image.png)

Now that we have described the theoretical grounds surrounding LSTMs, we can begin applying it in a practical setting. The memory component of LSTMs is incredibly useful for sequential data, where long term dependencies often need to be identified. One such case is Time Series data, where long term trends are of great interest, especially when short term volatility might distort certain relationships. 

We will work with some weather data from the Amsterdam region. This topic is of interest given the rise of severe weather conditions, which has been exacerbated by climate change. This makes LSTMs of great use as they can serve to capture long term trends in the weather, while also adjusting to new changes in weather data. 

We use historical data from OpenWeather to train the model as well as their api, in order to receive up to date weather information to use as an input to get a prediction from the trained model.

<h1>LSTM Model Weather Prediction</h1>


```python
# Import Libraries
import requests
import pandas as pd
import datetime as dt
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
import os
from sklearn.preprocessing import MinMaxScaler

```

    /Users/louis/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
      warnings.warn(
    2024-03-20 22:35:11.038376: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
    To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.


<h1>Data Cleaning and Visualization</h1>


```python
## Import training data and exploratory data analysis
df = pd.read_json('eefe6b469c04d45e458d32b93515cf34.json')
```


```python
#Extract Data from dictionaries into new columns
df['wind direction'] = df['wind'].apply(lambda x: x.get('deg'))
df['temperature'] = df['main'].apply(lambda x: x.get('temp'))
df['minimum temperature'] = df['main'].apply(lambda x: x.get('temp_min'))
df['maximum temperature'] = df['main'].apply(lambda x: x.get('temp_max'))
df['humidity'] = df['main'].apply(lambda x: x.get('humidity'))
df['pressure'] = df['main'].apply(lambda x: x.get('pressure'))
df['feels like'] = df['main'].apply(lambda x: x.get('feels_like'))
df['clouds'] = df['clouds'].apply(lambda x: x.get('all'))
df['wind speed'] = df['wind'].apply(lambda x: x.get('speed'))
```


```python
# Remove all unusual data points
df = df[df['minimum temperature'] > -20]
df = df[df['maximum temperature'] > -20]
df = df[df['temperature'] > -20]
df = df[df['feels like'] > -20]
```


```python
# Remove unecessary columns
df = df.drop(columns = ['dt','lat', 'lon', 'city_name', 'rain', 'timezone','snow','main','visibility', 'weather', 'wind', 'wind direction', 'clouds'])
# Format Date and Time
df['dt_iso'] = df['dt_iso'].str.replace(r'\s\+\d{4}\sUTC$', '', regex=True)
df['dt_iso'] = pd.to_datetime(df['dt_iso'])
```


```python
# We limit ourselves to data from 2010 onwards, to capture more recent weather developments.
df = df[df['dt_iso'].dt.year >= 2010]
```


```python
titles = ['Temperature', 'Minimum Temperature', 'Maximum Temperature', 'Humidity', 'pressure', 'Wind Speed',]
feature_keys = ['temperature', 'minimum temperature', 'maximum temperature', 'humidity', 'pressure', 'wind speed',]
colors = [
    "blue",
    "orange",
    "green",
    "red",
    "purple",
    "brown",
    "pink",
    "gray",
    "olive",
    "cyan",
]

date_time = 'dt_iso'

def show_raw_visualization(data):
    time_data = data[date_time]
    fig, axes = plt.subplots(
        nrows=3, ncols=2, figsize=(15, 20), dpi=80, facecolor="w", edgecolor="k"
    )
    for i in range(len(feature_keys)):
        key = feature_keys[i]
        c = colors[i % (len(colors))]
        t_data = data[key]
        t_data.index = time_data
        t_data.head()
        ax = t_data.plot(
            ax=axes[i // 2, i % 2],
            color=c,
            title="{} - {}".format(titles[i], key),
            rot=25,
        )

        ax.legend([titles[i]])
    plt.tight_layout()

show_raw_visualization(df)
```


    
![png](/images/output_19_0.png)
    



```python
# Heatmap to look at correlations
def show_heatmap(data):
    plt.matshow(data.corr())
    plt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=90)
    plt.gca().xaxis.tick_bottom()
    plt.yticks(range(data.shape[1]), data.columns, fontsize=14)

    cb = plt.colorbar()
    cb.ax.tick_params(labelsize=14)
    plt.title("Feature Correlation Heatmap", fontsize=14)
    plt.show()


show_heatmap(df)
```


    
![png](/images/output_20_0.png)
    


<h1>Training Model</h1>


```python
df = df.set_index('dt_iso')
```


```python
def df_to_X_y(df, window_size=5):
  df_as_np = df.to_numpy()
  X = []
  y = []
  for i in range(len(df_as_np)-window_size):
    row = [[a] for a in df_as_np[i:i+window_size]]
    X.append(row)
    label = df_as_np[i+window_size]
    y.append(label)
  return np.array(X), np.array(y)
```


```python
WINDOW_SIZE = 5
temp = df['temperature']
X1, y1 = df_to_X_y(temp, WINDOW_SIZE)
X1.shape, y1.shape
```




    ((119464, 5, 1), (119464,))




```python
X_train1, y_train1 = X1[:95000], y1[:95000]
X_val1, y_val1 = X1[95000:100000], y1[95000:100000]
X_test1, y_test1 = X1[95000:], y1[95000:]
X_train1.shape, y_train1.shape, X_val1.shape, y_val1.shape, X_test1.shape, y_test1.shape
```




    ((95000, 5, 1), (95000,), (5000, 5, 1), (5000,), (24464, 5, 1), (24464,))




```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.optimizers import Adam

model1 = Sequential()
model1.add(InputLayer((5, 1)))
model1.add(LSTM(64))
model1.add(Dense(8, 'relu'))
model1.add(Dense(1, 'linear'))

model1.summary()
```

    Model: "sequential"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     lstm (LSTM)                 (None, 64)                16896     
                                                                     
     dense (Dense)               (None, 8)                 520       
                                                                     
     dense_1 (Dense)             (None, 1)                 9         
                                                                     
    =================================================================
    Total params: 17425 (68.07 KB)
    Trainable params: 17425 (68.07 KB)
    Non-trainable params: 0 (0.00 Byte)
    _________________________________________________________________



```python
cp1 = ModelCheckpoint('model1/', save_best_only=True)
model1.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.0001), metrics=[RootMeanSquaredError()])
```


```python
model = model1.fit(X_train1, y_train1, validation_data=(X_val1, y_val1), epochs=10, callbacks=[cp1])
```

    Epoch 1/10
    2959/2969 [============================>.] - ETA: 0s - loss: 25.8520 - root_mean_squared_error: 5.0845INFO:tensorflow:Assets written to: model1/assets


    INFO:tensorflow:Assets written to: model1/assets


    2969/2969 [==============================] - 13s 4ms/step - loss: 25.7715 - root_mean_squared_error: 5.0766 - val_loss: 0.8417 - val_root_mean_squared_error: 0.9175
    Epoch 2/10
    2968/2969 [============================>.] - ETA: 0s - loss: 1.0231 - root_mean_squared_error: 1.0115INFO:tensorflow:Assets written to: model1/assets


    INFO:tensorflow:Assets written to: model1/assets


    2969/2969 [==============================] - 11s 4ms/step - loss: 1.0231 - root_mean_squared_error: 1.0115 - val_loss: 0.4154 - val_root_mean_squared_error: 0.6445
    Epoch 3/10
    2951/2969 [============================>.] - ETA: 0s - loss: 0.5549 - root_mean_squared_error: 0.7449INFO:tensorflow:Assets written to: model1/assets


    INFO:tensorflow:Assets written to: model1/assets


    2969/2969 [==============================] - 11s 4ms/step - loss: 0.5556 - root_mean_squared_error: 0.7454 - val_loss: 0.3135 - val_root_mean_squared_error: 0.5599
    Epoch 4/10
    2966/2969 [============================>.] - ETA: 0s - loss: 0.4812 - root_mean_squared_error: 0.6937INFO:tensorflow:Assets written to: model1/assets


    INFO:tensorflow:Assets written to: model1/assets


    2969/2969 [==============================] - 10s 4ms/step - loss: 0.4813 - root_mean_squared_error: 0.6937 - val_loss: 0.2939 - val_root_mean_squared_error: 0.5421
    Epoch 5/10
    2969/2969 [==============================] - 8s 3ms/step - loss: 0.4652 - root_mean_squared_error: 0.6821 - val_loss: 0.3006 - val_root_mean_squared_error: 0.5482
    Epoch 6/10
    2969/2969 [==============================] - 8s 3ms/step - loss: 0.4581 - root_mean_squared_error: 0.6768 - val_loss: 0.3001 - val_root_mean_squared_error: 0.5478
    Epoch 7/10
    2969/2969 [==============================] - 8s 3ms/step - loss: 0.4555 - root_mean_squared_error: 0.6749 - val_loss: 0.2993 - val_root_mean_squared_error: 0.5471
    Epoch 8/10
    2960/2969 [============================>.] - ETA: 0s - loss: 0.4537 - root_mean_squared_error: 0.6736INFO:tensorflow:Assets written to: model1/assets


    INFO:tensorflow:Assets written to: model1/assets


    2969/2969 [==============================] - 11s 4ms/step - loss: 0.4536 - root_mean_squared_error: 0.6735 - val_loss: 0.2897 - val_root_mean_squared_error: 0.5382
    Epoch 9/10
    2969/2969 [==============================] - 8s 3ms/step - loss: 0.4516 - root_mean_squared_error: 0.6720 - val_loss: 0.2924 - val_root_mean_squared_error: 0.5408
    Epoch 10/10
    2965/2969 [============================>.] - ETA: 0s - loss: 0.4502 - root_mean_squared_error: 0.6710INFO:tensorflow:Assets written to: model1/assets


    INFO:tensorflow:Assets written to: model1/assets


    2969/2969 [==============================] - 11s 4ms/step - loss: 0.4504 - root_mean_squared_error: 0.6711 - val_loss: 0.2842 - val_root_mean_squared_error: 0.5331



```python
#model1.save_weights('main_model_weights.h5')
model1.load_weights('main_model_weights.h5')
```


```python
from tensorflow.keras.models import load_model
model1 = load_model('model1/')
```


```python
history['loss'][1:]
```




    [1.0231046676635742,
     0.5555630326271057,
     0.48127850890159607,
     0.4651935398578644,
     0.45810866355895996,
     0.45551982522010803,
     0.45364946126937866,
     0.4515911042690277,
     0.45040321350097656]




```python
history = model.history
training_loss = history['loss']
validation_loss = history['val_loss']

epochs = range(1, len(history['loss']) + 1)
plt.plot(epochs, history['loss'], 'b', label='Training loss')
plt.title('Training Loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```


    
![png](/images/output_32_0.png)
    



```python
plt.plot(epochs, history['val_loss'], 'b', label='Validation loss')
plt.title('Validation Loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```


    
![png](/images/output_33_0.png)
    


Training loss decreases a lot over the epochs, where as there is some fluctation in the validation loss indicating potential overfitting.


```python
train_predictions = model1.predict(X_train1).flatten()
train_results = pd.DataFrame(data={'Train Predictions':train_predictions, 'Actuals':y_train1})
train_results
```

    2969/2969 [==============================] - 7s 2ms/step





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Train Predictions</th>
      <th>Actuals</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.560781</td>
      <td>0.20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.136540</td>
      <td>0.07</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.051392</td>
      <td>-0.06</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.055067</td>
      <td>-0.36</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.391277</td>
      <td>-0.16</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>94995</th>
      <td>16.444139</td>
      <td>17.07</td>
    </tr>
    <tr>
      <th>94996</th>
      <td>16.785131</td>
      <td>17.76</td>
    </tr>
    <tr>
      <th>94997</th>
      <td>17.845577</td>
      <td>17.49</td>
    </tr>
    <tr>
      <th>94998</th>
      <td>17.290297</td>
      <td>17.46</td>
    </tr>
    <tr>
      <th>94999</th>
      <td>17.180826</td>
      <td>17.11</td>
    </tr>
  </tbody>
</table>
<p>95000 rows × 2 columns</p>
</div>




```python
import matplotlib.pyplot as plt
plt.plot(train_results['Train Predictions'][:100], label = "Predicted")
plt.plot(train_results['Actuals'][:100], label = "Actual")
plt.title('Training Predications vs Actual')
plt.xlabel("Time")
plt.ylabel("Temperature C˚")
plt.legend()
```




    <matplotlib.legend.Legend at 0x13a0d9d30>




    
![png](/images/output_36_1.png)
    



```python
test_predictions = model1.predict(X_test1).flatten()
test_results = pd.DataFrame(data={'Test Predictions':test_predictions, 'Actuals':y_test1})
test_results
```

    765/765 [==============================] - 2s 2ms/step





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Test Predictions</th>
      <th>Actuals</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>16.700644</td>
      <td>15.60</td>
    </tr>
    <tr>
      <th>1</th>
      <td>14.855467</td>
      <td>14.20</td>
    </tr>
    <tr>
      <th>2</th>
      <td>13.325727</td>
      <td>13.36</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12.623538</td>
      <td>12.98</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12.526378</td>
      <td>12.95</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>24459</th>
      <td>22.173719</td>
      <td>22.47</td>
    </tr>
    <tr>
      <th>24460</th>
      <td>21.684772</td>
      <td>21.78</td>
    </tr>
    <tr>
      <th>24461</th>
      <td>20.951662</td>
      <td>21.72</td>
    </tr>
    <tr>
      <th>24462</th>
      <td>21.130159</td>
      <td>21.88</td>
    </tr>
    <tr>
      <th>24463</th>
      <td>21.517111</td>
      <td>21.68</td>
    </tr>
  </tbody>
</table>
<p>24464 rows × 2 columns</p>
</div>




```python
plt.plot(test_results['Test Predictions'][:100], label = "Predicted")
plt.plot(test_results['Actuals'][:100], label = "Actual")
plt.title('Validation Predications vs Actual')
plt.xlabel("Time")
plt.ylabel("Temperature C˚")
plt.legend()
```




    <matplotlib.legend.Legend at 0x13a801af0>




    
![png](/images/output_38_1.png)
    


<h1>Hyperparameter Tuning </h1>

In order to improve model performance we apply Hyperparameter Tuning using Keras' built in Tuner.


```python
from kerastuner.tuners import RandomSearch

def build_model(hp):
    model = Sequential()
    
    model.add(LSTM(units=hp.Int('units', min_value=32, max_value=256, step=32), input_shape=(5, 1))) 

    model.add(Dense(units=hp.Int('dense_units', min_value=4, max_value=64, step=4),activation='relu'))

    model.add(Dense(1, activation='linear'))

    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),loss='mean_squared_error',metrics=['mean_absolute_error'])

    return model

tuner = RandomSearch(build_model,objective='val_mean_absolute_error',max_trials=10,executions_per_trial=1,directory='my_dir',project_name='lstm_temperature_tuning')

tuner.search(X_train1, y_train1, epochs=10, validation_data=(X_val1, y_val1), callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=3)])

best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

```

    Trial 10 Complete [00h 00m 55s]
    val_mean_absolute_error: 0.37746018171310425
    
    Best val_mean_absolute_error So Far: 0.36918413639068604
    Total elapsed time: 00h 22m 36s



```python
# Get the best hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

best_model = tuner.hypermodel.build(best_hps)
```


```python
from tensorflow.keras.models import load_model

best_model = load_model('best_lstm_model.h5')

best_mod = best_model.fit(X_train1, y_train1, validation_data=(X_val1, y_val1), epochs=10)
```

    Epoch 1/10
    2969/2969 [==============================] - 11s 3ms/step - loss: 4.9430 - mean_absolute_error: 0.9221 - val_loss: 0.2997 - val_mean_absolute_error: 0.3854
    Epoch 2/10
    2969/2969 [==============================] - 9s 3ms/step - loss: 0.4742 - mean_absolute_error: 0.4701 - val_loss: 0.3016 - val_mean_absolute_error: 0.3962
    Epoch 3/10
    2969/2969 [==============================] - 10s 3ms/step - loss: 0.4633 - mean_absolute_error: 0.4650 - val_loss: 0.2783 - val_mean_absolute_error: 0.3737
    Epoch 4/10
    2969/2969 [==============================] - 10s 3ms/step - loss: 0.4556 - mean_absolute_error: 0.4596 - val_loss: 0.2785 - val_mean_absolute_error: 0.3707
    Epoch 5/10
    2969/2969 [==============================] - 10s 3ms/step - loss: 0.4511 - mean_absolute_error: 0.4572 - val_loss: 0.2856 - val_mean_absolute_error: 0.3807
    Epoch 6/10
    2969/2969 [==============================] - 10s 3ms/step - loss: 0.4505 - mean_absolute_error: 0.4576 - val_loss: 0.2811 - val_mean_absolute_error: 0.3780
    Epoch 7/10
    2969/2969 [==============================] - 10s 3ms/step - loss: 0.4468 - mean_absolute_error: 0.4558 - val_loss: 0.3025 - val_mean_absolute_error: 0.3911
    Epoch 8/10
    2969/2969 [==============================] - 10s 3ms/step - loss: 0.4455 - mean_absolute_error: 0.4544 - val_loss: 0.2765 - val_mean_absolute_error: 0.3712
    Epoch 9/10
    2969/2969 [==============================] - 10s 3ms/step - loss: 0.4444 - mean_absolute_error: 0.4531 - val_loss: 0.2768 - val_mean_absolute_error: 0.3707
    Epoch 10/10
    2969/2969 [==============================] - 10s 3ms/step - loss: 0.4432 - mean_absolute_error: 0.4536 - val_loss: 0.2823 - val_mean_absolute_error: 0.3749


We observe that the improvement in model performance is minimal as seen by the validation loss.

<h1>Multivariate Forecasting </h1>

We start by using multiple metrics such as humidity, pressure and wind speed to estimate the single metric temperature. After this we expand to predicting more than just temperature.


```python
multi_input = pd.concat([df['temperature'], df['humidity'], df['pressure'], df['wind speed']], axis =1 )
multi_input
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temperature</th>
      <th>humidity</th>
      <th>pressure</th>
      <th>wind speed</th>
    </tr>
    <tr>
      <th>dt_iso</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2010-01-01 00:00:00</th>
      <td>0.74</td>
      <td>84</td>
      <td>998</td>
      <td>7.00</td>
    </tr>
    <tr>
      <th>2010-01-01 01:00:00</th>
      <td>0.72</td>
      <td>85</td>
      <td>998</td>
      <td>8.00</td>
    </tr>
    <tr>
      <th>2010-01-01 02:00:00</th>
      <td>0.65</td>
      <td>84</td>
      <td>998</td>
      <td>7.00</td>
    </tr>
    <tr>
      <th>2010-01-01 03:00:00</th>
      <td>0.61</td>
      <td>81</td>
      <td>998</td>
      <td>8.00</td>
    </tr>
    <tr>
      <th>2010-01-01 04:00:00</th>
      <td>0.54</td>
      <td>80</td>
      <td>999</td>
      <td>7.00</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2023-08-18 19:00:00</th>
      <td>22.47</td>
      <td>87</td>
      <td>1011</td>
      <td>2.57</td>
    </tr>
    <tr>
      <th>2023-08-18 20:00:00</th>
      <td>21.78</td>
      <td>90</td>
      <td>1012</td>
      <td>2.06</td>
    </tr>
    <tr>
      <th>2023-08-18 21:00:00</th>
      <td>21.72</td>
      <td>90</td>
      <td>1011</td>
      <td>2.57</td>
    </tr>
    <tr>
      <th>2023-08-18 22:00:00</th>
      <td>21.88</td>
      <td>90</td>
      <td>1011</td>
      <td>3.09</td>
    </tr>
    <tr>
      <th>2023-08-18 23:00:00</th>
      <td>21.68</td>
      <td>90</td>
      <td>1011</td>
      <td>4.12</td>
    </tr>
  </tbody>
</table>
<p>119469 rows × 4 columns</p>
</div>




```python
# Take multiple inputs to one output (temperature), raise window size to 8 
def df_to_X_y2(df, window_size=8):
  df_as_np = df.to_numpy()
  X = []
  y = []
  for i in range(len(df_as_np)-window_size):
    row = [r for r in df_as_np[i:i+window_size]]
    X.append(row)
    label = [df_as_np[i+window_size][0]]
    y.append(label)
  return np.array(X), np.array(y)
```


```python
x2, y2 = df_to_X_y2(multi_input)
x2.shape, y2.shape
```




    ((119461, 8, 4), (119461, 1))




```python
X_train2, y_train2 = x2[:95000], y2[:95000]
X_val2, y_val2 = x2[95000:100000], y2[95000:100000]
X_test2, y_test2 = x2[95000:], y2[95000:]
X_train2.shape, y_train2.shape, X_val2.shape, y_val2.shape, X_test2.shape, y_test2.shape
```




    ((95000, 8, 4), (95000, 1), (5000, 8, 4), (5000, 1), (24461, 8, 4), (24461, 1))



(Training set length, window size, variables), (Training set length, no. )


```python
temp_training_mean = np.mean(X_train2[:,:,0])
temp_training_sd = np.std(X_train2[:,:,0])

humid_training_mean = np.mean(X_train2[:,:,1])
humid_training_sd = np.std(X_train2[:,:,1])

press_training_mean = np.mean(X_train2[:,:,2])
press_training_sd = np.std(X_train2[:,:,2])

wind_training_mean = np.mean(X_train2[:,:,3])
wind_training_sd = np.std(X_train2[:,:,3])
```


```python
def process_input(X):
    X[:,:,0] = (X[:,:,0] - temp_training_mean) / temp_training_sd
    X[:,:,1] = (X[:,:,1] - humid_training_mean) / humid_training_sd
    X[:,:,2] = (X[:,:,2] - press_training_mean) / press_training_sd
    X[:,:,3] = (X[:,:,3] - wind_training_mean) / wind_training_sd
    return X
    
def process_output(y):
    y[:,0] = (y[:,0] - temp_training_mean) / temp_training_sd
    return y 
```


```python
process_input(X_train2)
process_input(X_val2)
process_input(X_test2)

process_output(y_train2)
process_output(y_val2)
process_output(y_test2)
```




    array([[-1.65787907],
           [-1.65859917],
           [-1.66243968],
           ...,
           [ 1.65982617],
           [ 1.68461492],
           [ 1.65362898]])




```python
y_train2
```




    array([[-1.76102205],
           [-1.73003611],
           [-1.68045859],
           ...,
           [ 0.71165628],
           [ 0.49475467],
           [ 0.36461371]])




```python
# 5->8, new window size. 1->4, 4 inputs
model2 = Sequential()
model2.add(InputLayer((8, 4)))
model2.add(LSTM(64))
model2.add(Dense(8, 'relu'))
model2.add(Dense(1, 'linear'))

model2.summary()
```

    Model: "sequential_1"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     lstm_1 (LSTM)               (None, 64)                17664     
                                                                     
     dense_2 (Dense)             (None, 8)                 520       
                                                                     
     dense_3 (Dense)             (None, 1)                 9         
                                                                     
    =================================================================
    Total params: 18193 (71.07 KB)
    Trainable params: 18193 (71.07 KB)
    Non-trainable params: 0 (0.00 Byte)
    _________________________________________________________________



```python
cp2 = ModelCheckpoint('model2/', save_best_only=True)
model2.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.0001), metrics=[RootMeanSquaredError()])
```

To be continued...
